https://www.youtube.com/watch?v=nATRPPZ5dGE   // setup nvdia for windows

https://medium.com/@piyushbatra1999/installing-llama-cpp-python-with-nvidia-gpu-acceleration-on-windows-a-short-guide-0dfac475002d           # llama.cpp using the nvdia gpus

https://llama-cpp-python.readthedocs.io/en/latest/                                                   # llama cpp documentation ***
https://huggingface.co/docs/api-inference/tasks/chat-completion                                       # create_chat_completion docs ***

https://freedium.cfd/https://medium.com/@gonchogo/how-to-install-llama-cpp-with-cuda-on-windows-05008d14f603   # llama-cpp using the gpu
https://github.com/langchain-ai/langchain/discussions/25342                                                    # llama-cpp using the gpu




## Llama.cpp
Llama.cpp is known for its portability and efficiency designed to run optimally on CPUs and GPUs without requiring specialized hardware. It is a lightweight framework which makes it ideal for technology teams launching LLMs on smaller devices and local On-Prem machines such as Edge use case scenarios.


## vLLM
In contrast to Llama.cpp, vLLM focuses on ease of use and performance, offering a more streamlined experience with fewer customization requirements. vLLM leverages GPU acceleration to achieve higher performance making it more suitable for environments with access to powerful GPU